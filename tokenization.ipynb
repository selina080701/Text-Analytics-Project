{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd7f1364df4739",
   "metadata": {},
   "source": [
    "# Tokenization: Genius Song Lyrics Subset (1%)\n",
    "\n",
    "**Dataset:** 34'049 Songs | 26'408 Artists | 6 Genres\n",
    "\n",
    "**Purpose:** Prepare the text data for further analysis by splitting the song lyrics into individual tokens and removing stopwords. These preprocessing steps ensure that the data is clean and structured, providing a solid foundation for subsequent analyses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f005ba3cadd89ec",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "## 1.1 Import Packages and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33644d79048ac15b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:47.245828Z",
     "start_time": "2025-10-23T18:34:47.241639Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import tee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc58bbdc538f8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:47.272323Z",
     "start_time": "2025-10-23T18:34:47.267876Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b037bfd5f24edd",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927633b05329028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:48.580375Z",
     "start_time": "2025-10-23T18:34:47.297232Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean/lyrics_subset_1pct_clean.csv')\n",
    "\n",
    "# select only english songs -> better for analysis\n",
    "df = df[df[\"language_cld3\"] == \"en\"]\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Number of Songs: {len(df)} | Artists: {df['artist'].nunique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955df61f8357c8f",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "## 2.1 Build Token\n",
    "\n",
    "First, the song lyrics are split into individual tokens, meaning that each word is extracted from the complete text. This allows for detailed analysis of word usage and frequency patterns in the lyrics. The resulting tokens are stored in the dataframe as a new column `words`, and an additional column `word_counts` is created to record the number of tokens per song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cab2a26010ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:48.700298Z",
     "start_time": "2025-10-23T18:34:48.695016Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, lowercase=True):\n",
    "    \"\"\"Clean and tokenize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "preprocess_text(\"This is a test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ace47fad1b304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:51.010846Z",
     "start_time": "2025-10-23T18:34:48.750056Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"words\"] = df[\"lyrics\"].apply(preprocess_text)\n",
    "df[\"word_count\"] = df[\"words\"].apply(len) # words per song\n",
    "\n",
    "df[['title','artist','words', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe08d3166ea0a98",
   "metadata": {},
   "source": [
    "## 2.2 Filter Stopwords\n",
    "After splitting the text into tokens, stopwords are filtered and removed. This step ensures a more meaningful analysis by excluding common words that do not contribute significant semantic value to the lyrics. The resulting tokens are stored in the dataframe as a new column `tokens`, and an additional column `token_count` is created to record the number of tokens per song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9c5325e121fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:51.093252Z",
     "start_time": "2025-10-23T18:34:51.086143Z"
    }
   },
   "outputs": [],
   "source": [
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"if\",\"then\",\"so\",\"than\",\"that\",\"those\",\"these\",\"this\",\n",
    "    \"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"as\",\"at\",\"by\",\"from\",\"into\",\"over\",\"under\",\"up\",\"down\",\n",
    "    \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"do\",\"does\",\"did\",\"doing\",\"have\",\"has\",\"had\",\n",
    "    \"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\"my\",\"your\",\"his\",\"its\",\"our\",\"their\",\n",
    "    \"not\",\"no\",\"yes\",\"yeah\",\"y'all\",\"yall\",\"im\",\"i'm\",\"i’d\",\"i'd\",\"i’ll\",\"i'll\",\"youre\",\"you're\",\"dont\",\"don't\",\n",
    "    \"cant\",\"can't\",\"ill\",\"i’ll\",\"id\",\"i'd\",\"ive\",\"i’ve\",\"ya\",\"oh\",\"ooh\",\"la\",\"na\",\"nah\"\n",
    "}\n",
    "\n",
    "def filtered_tokens(text):\n",
    "    \"\"\"filter stopwords\"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    return [t for t in tokens if t not in STOPWORDS and not t.isdigit() and len(t) > 1]\n",
    "\n",
    "filtered_tokens(\"This is a test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175aaca3a877ea59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:54.453452Z",
     "start_time": "2025-10-23T18:34:51.153553Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"lyrics\"].apply(filtered_tokens)\n",
    "tokens_per_row = df[\"tokens\"]\n",
    "tokens = [t for row in tokens_per_row for t in row]\n",
    "df[\"token_count\"] = df[\"tokens\"].apply(len)\n",
    "\n",
    "df[['title','artist', 'words','word_count','tokens','token_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc28932b0f228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:58.146431Z",
     "start_time": "2025-10-23T18:34:54.711846Z"
    }
   },
   "outputs": [],
   "source": [
    "top_n = 15\n",
    "words = [t for row in df[\"words\"] for t in row]\n",
    "tokens_filtered = [t for row in df[\"tokens\"] for t in row]\n",
    "\n",
    "word_counts_raw = Counter(words).most_common(top_n)\n",
    "word_counts_filtered = Counter(tokens_filtered).most_common(top_n)\n",
    "\n",
    "df_raw = pd.DataFrame(word_counts_raw, columns=[\"word\", \"count\"])\n",
    "df_filtered = pd.DataFrame(word_counts_filtered, columns=[\"word\", \"count\"])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "axes[0].bar(df_raw[\"word\"], df_raw[\"count\"])\n",
    "axes[0].set_title(f\"Top {top_n} Words (Before Stopword Removal)\", fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "axes[1].bar(df_filtered[\"word\"], df_filtered[\"count\"])\n",
    "axes[1].set_title(f\"Top {top_n} Words (After Stopword Removal)\", fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2aed497e9679f",
   "metadata": {},
   "source": [
    "The plots above show the frequency of the 15 most common words before and after removing stopwords. We can clearly see that removing stopwords makes a significant difference: the most frequent word after filtering does not even appear among the top 15 words before stopword removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed1631a96614c6",
   "metadata": {},
   "source": [
    "# 3. Save final Dataset\n",
    "The final dataset with the new columns `tokens`, `token_count`, `words` and `word_count` is shown below. It serves as the cleaned and preprocessed version of the original data, ready for subsequent text analysis steps. Finally, the dataset is saved locally under `data/clean/data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8676317173483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:58.216954Z",
     "start_time": "2025-10-23T18:34:58.201102Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19c136c41eb5ff",
   "metadata": {},
   "source": [
    "## 3.1 Configuration\n",
    "\n",
    "* Define output directory and file name.\n",
    "* Create directory if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de08e4603233ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:34:59.302188Z",
     "start_time": "2025-10-23T18:34:59.298050Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"data/clean\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7989463c912091f",
   "metadata": {},
   "source": [
    "## 3.2 Save Dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467705f6b19f2110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T18:35:04.962554Z",
     "start_time": "2025-10-23T18:34:59.376372Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned Subset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
